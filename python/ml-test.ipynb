{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a chatbot using DialoGPT-large\r\n",
    "Fine-tune."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\r\n",
    "import torch\r\n",
    "\r\n",
    "\r\n",
    "def load_tokenizer_and_model(model=\"microsoft/DialoGPT-large\"):\r\n",
    "  \"\"\"\r\n",
    "    Load tokenizer and model instance for some specific DialoGPT model.\r\n",
    "  \"\"\"\r\n",
    "  # Initialize tokenizer and model\r\n",
    "  print(\"Loading model...\")\r\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\r\n",
    "  model = AutoModelForCausalLM.from_pretrained(model)\r\n",
    "  \r\n",
    "  # Return tokenizer and model\r\n",
    "  return tokenizer, model\r\n",
    "\r\n",
    "\r\n",
    "def generate_response(tokenizer, model, chat_round, chat_history_ids):\r\n",
    "  \"\"\"\r\n",
    "    Generate a response to some user input.\r\n",
    "  \"\"\"\r\n",
    "  # Encode user input and End-of-String (EOS) token\r\n",
    "  new_input_ids = tokenizer.encode(input(\">> You:\") + tokenizer.eos_token, return_tensors='pt')\r\n",
    "\r\n",
    "  # Append tokens to chat history\r\n",
    "  bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_round > 0 else new_input_ids\r\n",
    "\r\n",
    "  # Generate response given maximum chat length history of 1250 tokens\r\n",
    "  chat_history_ids = model.generate(bot_input_ids, max_length=1250, pad_token_id=tokenizer.eos_token_id)\r\n",
    "  \r\n",
    "  # Print response\r\n",
    "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\r\n",
    "  \r\n",
    "  # Return the chat history ids\r\n",
    "  return chat_history_ids\r\n",
    "\r\n",
    "\r\n",
    "def chat_for_n_rounds(n=5):\r\n",
    "  \"\"\"\r\n",
    "  Chat with chatbot for n rounds (n = 5 by default)\r\n",
    "  \"\"\"\r\n",
    "  \r\n",
    "  # Initialize tokenizer and model\r\n",
    "  tokenizer, model = load_tokenizer_and_model()\r\n",
    "  \r\n",
    "  # Initialize history variable\r\n",
    "  chat_history_ids = None\r\n",
    "  \r\n",
    "  # Chat for n rounds\r\n",
    "  for chat_round in range(n):\r\n",
    "    chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "  chat_for_n_rounds(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model...\n",
      "DialoGPT: I'm doing great, thank you for asking.\n",
      "DialoGPT: I think it's a good idea.\n",
      "DialoGPT: I am not, but I am willing to try.\n",
      "DialoGPT: I am not a robot, but I am a robot.\n",
      "DialoGPT: I am not a robot, but I am a robot.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tune the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer, model = load_tokenizer_and_model()\r\n",
    "\r\n",
    "AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT\")\r\n",
    "# Load dictionary of new training examples"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}